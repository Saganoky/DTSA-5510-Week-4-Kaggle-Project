{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4d294e8",
   "metadata": {},
   "source": [
    "# DTSA-5510 Week 4: Part 2\n",
    "## Author: Alan Klein\n",
    "## Create Date: 2025-04-27\n",
    "github link: https://github.com/Saganoky/DTSA-5510-Week-4-Kaggle-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f11b59",
   "metadata": {},
   "source": [
    "## Part 2: Questions\n",
    "\n",
    "1. Load the movie ratings data (as in the HW3-recommender-system) and use matrix factorization technique(s) and predict the missing ratings from the test data. Measure the RMSE. You should use sklearn library. [10 pts]\n",
    "\n",
    "The performance was terrible, having a RMSE of 2.85 for both the test and training data.\n",
    "\n",
    "\n",
    "2. Discuss the results and why sklearn's non-negative matrix factorization library did not work well compared to simple baseline or similarity-based methods weâ€™ve done in Module 3. Can you suggest a way(s) to fix it? [10 pts]\n",
    "\n",
    "With 50 iterations, this process is taking 5 minutes to run and its not reaching convergence, if we increased the iterations it could get better results.  You could try and apply some spare data techniques to increase performance.  Additionally, we could try regularization or PCA to reduce the dimensionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3df75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from collections import namedtuple\n",
    "\n",
    "from sklearn.metrics import accuracy_score, root_mean_squared_error\n",
    "from scipy.sparse import coo_matrix\n",
    "from sklearn.decomposition import NMF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c79ffd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MV_users = pd.read_csv('data/part_2/users.csv')\n",
    "MV_movies = pd.read_csv('data/part_2/movies.csv')\n",
    "train = pd.read_csv('data/part_2/train.csv')\n",
    "test = pd.read_csv('data/part_2/test.csv')\n",
    "\n",
    "Data = namedtuple('Data', ['users','movies','train','test'])\n",
    "data = Data(MV_users, MV_movies, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dfcf18ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the data to be used for NMF, needs to be user ratings and movie genres for each row.\n",
    "# Copy code from week 3 assignment\n",
    "\n",
    "allusers = list(data.users['uID'])\n",
    "allmovies = list(data.movies['mID'])\n",
    "genres = list(data.movies.columns.drop(['mID', 'title', 'year']))\n",
    "mid2idx = dict(zip(data.movies.mID,list(range(len(data.movies)))))\n",
    "uid2idx = dict(zip(data.users.uID,list(range(len(data.users)))))\n",
    "\n",
    "ind_movie = [mid2idx[x] for x in data.train.mID] \n",
    "ind_user = [uid2idx[x] for x in data.train.uID]\n",
    "rating_train = list(data.train.rating)\n",
    "Movie_Ratings = np.array(coo_matrix((rating_train, (ind_user, ind_movie)), shape=(len(allusers), len(allmovies))).toarray())\n",
    "\n",
    "\n",
    "# Create wide dataframe\n",
    "uid2idx_df = pd.DataFrame(list(uid2idx.items()), columns=['uID', 'index'])\n",
    "\n",
    "movie_ratings_df = pd.DataFrame(Movie_Ratings)\n",
    "movie_ratings_df['index'] = list(range(len(movie_ratings_df)))\n",
    "movie_ratings_df = pd.merge(movie_ratings_df, uid2idx_df, on='index')\n",
    "\n",
    "train_enriched_1 = pd.merge(data.train, data.users, on='uID')\n",
    "train_enriched_2 = pd.merge(train_enriched_1, data.movies, on='mID')\n",
    "train_enriched_3 = pd.merge(train_enriched_2, movie_ratings_df, on='uID')\n",
    "\n",
    "# Get X_train and y_train\n",
    "X_train = np.array(train_enriched_3.drop(columns=['uID', 'mID', 'rating',\n",
    "                                          'gender', 'occupation','zip', \n",
    "                                          'title','year','index']))\n",
    "y_train = np.array(train_enriched_3['rating'])\n",
    "\n",
    "\n",
    "# Enrich test set\n",
    "test_enriched_1 = pd.merge(data.test, data.users, on='uID')\n",
    "test_enriched_2 = pd.merge(test_enriched_1, data.movies, on='mID')\n",
    "test_enriched_3 = pd.merge(test_enriched_2, movie_ratings_df, on='uID')\n",
    "\n",
    "# Get X_test and y_test\n",
    "X_test = np.array(test_enriched_3.drop(columns=['uID', 'mID', 'rating',\n",
    "                                          'gender', 'occupation','zip', \n",
    "                                          'title','year','index']))    \n",
    "y_test = np.array(test_enriched_3['rating'])  \n",
    "\n",
    "\n",
    "# Not going to normalizing the data, since we are using NMF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2fa3379",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ajkle\\Development\\DTSA-5510-Week-4-Kaggle-Project\\.venv\\Lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 50 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best permutation: (0, 4, 3, 1, 2)\n",
      "Best RMSE: 2.84590445599657\n",
      "Best accuracy: 0.10517520631411163\n",
      "Train labels: [5 4 5 ... 3 3 1]\n",
      "Best permutation: (0, 4, 3, 1, 2)\n",
      "Best RMSE: 2.8512978427016757\n",
      "Best accuracy: 0.10539786644804591\n",
      "Train labels: [4 5 3 ... 4 3 4]\n"
     ]
    }
   ],
   "source": [
    "def create_nmf_model(feature_data):\n",
    "    nmf_model = NMF(n_components=5, random_state=1337, max_iter=50)\n",
    "    W = nmf_model.fit_transform(feature_data)\n",
    "    H = nmf_model.components_\n",
    "    return W, H, nmf_model\n",
    "\n",
    "def label_permute_compare(labels,W,n=5):\n",
    "    pred = np.argmax(W, axis = 1)\n",
    "    pred_pd = pd.DataFrame(pred)\n",
    "    best_acc = 0\n",
    "    best_rmse = 0\n",
    "    best_perm = None\n",
    "   \n",
    "    for perm in itertools.permutations(range(n)):\n",
    "        perm_yp = pred_pd.replace(list(range(n)), list(perm))\n",
    "        perm_rmse = root_mean_squared_error(labels, perm_yp)\n",
    "        perm_acc = accuracy_score(labels, perm_yp)\n",
    "        if perm_rmse > best_rmse:\n",
    "            best_rmse = perm_rmse\n",
    "            best_perm = perm\n",
    "            best_acc = perm_acc\n",
    "    return best_perm, best_acc, best_rmse\n",
    "\n",
    "def print_results(best_perm, best_rmse, best_acc, train_labs):\n",
    "    print(\"Best permutation:\", best_perm)\n",
    "    print(\"Best RMSE:\", best_rmse)\n",
    "    print(\"Best accuracy:\", best_acc)\n",
    "    print(\"Train labels:\", train_labs)\n",
    "\n",
    "W, H, nmf_model = create_nmf_model(X_train)\n",
    "best_perm, best_acc, best_rmse = label_permute_compare(y_train,W,n=5)\n",
    "print_results(best_perm, best_rmse, best_acc, y_train)\n",
    "\n",
    "# run on validation set\n",
    "w_val = nmf_model.transform(X_test) \n",
    "val_best_perm, val_best_acc, val_best_rmse = label_permute_compare(y_test,w_val,n=5)\n",
    "print_results(val_best_perm, val_best_rmse, val_best_acc, y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
